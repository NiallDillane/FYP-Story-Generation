% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Implementation} % top level followed by section, subsection


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{4_chapter4/figures/PNG/}{4_chapter4/figures/PDF/}{4_chapter4/figures/}}
\else
    \graphicspath{{4_chapter4/figures/EPS/}{4_chapter4/figures/}}
\fi


% ----------------------- contents from here ------------------------

\section{Web Scraping}

Gathered training data from reddit, few different approaches, different sized datasets produced.

As outlined in Section~\ref{data}, we decided to utilise the \href{https://www.reddit.com/r/nosleep}{No Sleep} (https://www.reddit.com/r/nosleep) community on reddit as a data source, owing to its focused nature on horror stories and mass of posts. Reddit does offer API support itself, so naturally this was the first route we took.

\subsection{Reddit API and PRAW}

Limited to 1000 posts

The \href{https://www.reddit.com/dev/api/}{Reddit API} (https://www.reddit.com/dev/api/) is freely available to developers, and offers plenty of functionality and customisation, not limited to: date ranges, filtering based on points, removing posts by certain users, etc. It perhaps isn't the easiest to use, however, but there also happen to be many API wrappers available â€“ effectively providing a layer of abstraction and a more user-friendly interface over the core API.

\href{https://praw.readthedocs.io/en/latest/}{PRAW} (https://praw.readthedocs.io/en/latest/) (Python Reddit API Wrapper) provides, as the name suggests, a Python interface, together with plenty of documentation and examples. we initially ran scripts like Listing~\ref{prawscrape}, which would scan through the top 1000 posts in a subreddit, add the "post" objects into a list, and then iterate over that list to add all of the posts' content into a text file. 

\lstinputlisting[fontadjust=\true, float, firstline=16,lastline=26, caption=PRAW Scrape Script, label={prawscrape}]
{../../../../code/data/scripts/praw_scrape.py} 

However, we soon ran into a problem, which was that the limit on post requests at a time was indeed 1000. This meant that going through in one call was going to be impossible or very messy with this API approach.

\subsection{PushShift API}

\href{https://pushshift.io/}{PushShift} (https://pushshift.io/) is a third-party data source which collects and collates reddit data into its own databse. It also provides an API, allowing us to query that with no artificial post limit. This was done with a more traditional http request as seen in Listing~\ref{pushscrape}.

\lstinputlisting[fontadjust=\true, float, caption=PushShift Scrape Script, label={pushscrape}]
{../../../../code/data/scripts/pushshift_scrape.py} 


\subsection{Datasets}

Small for testing, large unwieldy, medium used. Found that perplexity score was worse with medium compared to small.


\section{Training the Model}

Worried about hardware, online resources to the rescue! Sample scripts for GPT-2 provided by huggingface

\subsection{HuggingFace}

Open source abstraction of GPT-2, convenient scripts that can be modified and used. Checkpoints stored at various points.

\subsubsection{Scripts}

script code and explanation

\subsection{Google Colab}

Online jupyter notebook environment, can use a hosted runtime to take advantage of GPU/TPU, free for 12 hours at a time. 

\subsubsection{Workflow}

Results stored in runtime and can be exported to Google Drive and downloaded (checkpoints useful here).


\section{Python Flask API}

API to encapsulate scripts, easily callable and customisable.


\section{React JS}

Web framework, modern JavaScript library for building UI.

\subsection{Hooks}

Functional approach, no classes, using state which is passed around. Challenging new way of thinking but extensible and clean.


%\lstset{language=Python}
%\lstset{basicstyle=\tiny, 
%breaklines=true, 
%rulesepcolor=\color{c++green}, 
%keywordstyle=\color{c++purple}\bfseries, 
%stringstyle=\color{c++red}, 
%commentstyle=\color{c++green},}  
%
%%Load your code file below
%\lstinputlisting[fontadjust=\true]{ahrs_triad_mote.c}


% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
