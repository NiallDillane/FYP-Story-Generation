% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Implementation} % top level followed by section, subsection


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{4_implementation/figures/PNG/}{4_implementation/figures/PDF/}{4_implementation/figures/}}
\else
    \graphicspath{{4_implementation/figures/EPS/}{4_implementation/figures/}}
\fi


% ----------------------- contents from here ------------------------

\section{Web Scraping}

Gathered training data from reddit, few different approaches, different sized datasets produced.

As outlined in Section~\ref{data}, we decided to utilise the \href{https://www.reddit.com/r/nosleep}{No Sleep} (https://www.reddit.com/r/nosleep) community on reddit as a data source, owing to its focused nature on horror stories and mass of posts. Reddit does offer API support itself, so naturally this was the first route we took.

\subsection{Reddit API and PRAW}

Limited to 1000 posts

The \href{https://www.reddit.com/dev/api/}{Reddit API} (https://www.reddit.com/dev/api/) is freely available to developers, and offers plenty of functionality and customisation, not limited to: date ranges, filtering based on points, removing posts by certain users, etc. It perhaps isn't the easiest to use, however, but there also happen to be many API wrappers available â€“ effectively providing a layer of abstraction and a more user-friendly interface over the core API.

\href{https://praw.readthedocs.io/en/latest/}{PRAW} (https://praw.readthedocs.io/en/latest/) (Python Reddit API Wrapper) provides, as the name suggests, a Python interface, together with plenty of documentation and examples. we initially ran scripts like Listing~\ref{prawscrape}, which would scan through the top 1000 posts in a subreddit, add the "post" objects into a list, and then iterate over that list to add all of the posts' content into a text file. 

\lstinputlisting[fontadjust=\true, float, firstline=16,lastline=26, caption=PRAW Scrape Script, label={prawscrape}]
{../../../../code/data/scripts/praw_scrape.py} 

However, we soon ran into a problem, which was that the limit on post requests at a time was indeed 1000. This meant that going through in one call was going to be impossible or very messy with this API approach.

\subsection{PushShift API}

\href{https://pushshift.io/}{PushShift} (https://pushshift.io/) is a third-party data source which collects and collates reddit data into its own databse. It also provides an API, allowing us to query that with no artificial post limit. This was done with a more traditional http request as seen in Listing~\ref{pushscrape}. PushShift was slower but unlimited, meaning we could simply loop through pages of results (starting with the most recent), keeping track of the last post's creation date (line 23) and make sure that our next iteration precluded posts after that date. This took time but allowed us to assemble a variety of datasets, as discussed in Section~\ref{datasets}.

\lstinputlisting[fontadjust=\true, float, firstline=10, caption=PushShift Scrape Script, label={pushscrape}]
{../../../../code/data/scripts/pushshift_scrape.py} 


\subsection{Datasets} \label{datasets}

As discussed in Section~\ref{data}, we wished only to collect posts with a relatively high score as voted by the users of the community, and several datasets were assembled on a trial-and-error basis.

\begin{itemize}

  \item Initially, with the official Reddit API, a file with the top 1000 posts was created, a size of 12MB.
  \item Using PushShift, we expanded our scope and collated all posts with over 1000 points, for a size of 45.4MB.
  \item Feeling this wasn't enough, the points threshold was reduced to 400, which gave us a file size of 102.5MB.
  \item As a stretch goal, we set a very low bound on points, only 10 points, which created a file of 496.3MB.

\end{itemize}

However, when training these models individually for testing purposes, we found that perplexity (a measure for evalutating text generation, which will be discussed more in Section~\ref{evaluation}) actually worsened when moving from the first to the third dataset. As such, we decided to stop training and proceed with the third model, despite its slightly worse score, fearing the exponential increase in size from the fourth dataset.


\section{Training the Model}

Worried about hardware, online resources to the rescue! Sample scripts for GPT-2 provided by huggingface

The training process for these models revolved around Google Colab, as designed in Section~\ref{training}. Here, we were able to connect the Python Notebook environment to Google Drive storage, where we uploaded the relevant scripts for training as well as the datasets from which we were building the models. The environment looked something like Figure~\ref{colab_env}. We can see the file explorer on the left-hand-side, which allows access to Google Drive files via the /gdrive/ folder, and on the right we see the Notebook itself. Along the top toolbar we have menus including Runtime, which is where we connect to a remote hardware accelerator (Nvidia K80 GPU).

\figuremacroW{colab_env}{Google Colab Environment}{Python Notebook}{1}

Initial setup code involved mounting Google Drive and importing various required packages, as in Listing~\ref{colab_setup}. As you can see, we were able to run scripts and typical unix terminal commands using the (!) prefix.

\lstinputlisting[fontadjust=\true, float, firstline=12, lastline=22, caption=Colab Setup, label={colab_setup}]
{../../../../code/data/scripts/hf_transformers_train.py} 

Then, we utilised the run\_language\_modeling.py script, provided as part of the Huggingface implementation (discussed in Section~\ref{modelImpl}), to carry out the main training process. This took a variety of parameters, notably: the input files for training and evaluation, the number of training epochs to perform, how often to save checkpoints and whether those should be resumed, and finally the output directory. A sample of how we used that script is in Listing~\ref{colab_train}. This "output" foler with our model could then be copied back over to Google Drive and downloaded.

\lstinputlisting[fontadjust=\true, float, firstline=28, lastline=42, caption=Colab Setup, label={colab_train}]
{../../../../code/data/scripts/hf_transformers_train.py} 

Checkpoints were stored at steps specified in the script, into folders in the same file system. These were very useful in resuming previous training progress, since it did take several hours at a time and Colab imposes a 12 hour time consecutive runtime limit, with a cooldown period between.


\section{React JS}

Web framework, modern JavaScript library for building UI.

The front end of our application was written in JavaScript, specifically using the React library we mentioned in Section~\ref{frontend}. From a software design point of view, we went for a purely functional style with no classes, as these have become less relevant in newer versions of React. We felt this would aid the development process by virtue of being highly modular and adaptable.

Code etc etc.

\subsection{Hooks}

Functional approach, no classes, using state which is passed around. Challenging new way of thinking but extensible and clean.


\section{Python Flask API}

API to encapsulate scripts, easily callable and customisable.

The Python Flask API (outlined in Section~\ref{backend}) would utilise.



% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
