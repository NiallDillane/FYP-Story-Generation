% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Design} % top level followed by section, subsection

\ifpdf
    \graphicspath{{3_design/figures/PNG/}{3_design/figures/PDF/}{3_design/figures/}}
\else
    \graphicspath{{3_design/figures/EPS/}{3_design/figures/}}
\fi

% ----------------------- contents from here ------------------------




\section{Language Model}

We identified a range of potential models for this project, ranging from earlier grammars, to more recent Recurrent Neural Networks, and finally Transformers. For demonstration purposes, we decided it would be best to settle on one model.

\subsection{GPT-2}

As discussed in Section~\ref{gpt}, GPT-2 is currently the state-of-the-art model when it comes to the generation of natural language. A number of resources were identified which made it easier to utilise and fine-tune this model \footnote{\href{https://talktotransformer.com/}{Talk to Transformer} (https://talktotransformer.com/) by Adam King} \footnote{\href{https://github.com/minimaxir/gpt-2-simple}{GPT-2 Simple} (https://github.com/minimaxir/gpt-2-simple) by Max Woolf}, as well as the original open-source code itself \footnote{\href{https://github.com/openai/gpt-2/}{GPT-2} (https://github.com/openai/gpt-2/) by OpenAI}. 

This made the task somewhat less complex, but our original intention at the time was to modify the underlying architecture in some way. Training a model, we reasoned, was too basic, and we needed something more. Since the code is open source, we set about reading through it, along with many explainer articles \footnote{\href{http://jalammar.github.io/illustrated-gpt2/}{The Illustrated GPT-2} (http://jalammar.github.io/illustrated-gpt2/) by Jay Alammar} and videos \footnote{\href{https://youtu.be/S0KakHcj_rs}{Attention Is All You Need} (https://youtu.be/S0KakHcj\_rs/) by the AI Socratic Circles, was helpful in understanding the Attention mechanism}.

However, the enormity of this challenge was soon realised. These were concepts and models developed by teams of post-doctorate level researchers, working full-time on these problems, with the backing of huge corporations. For myself, just getting started in the field of machine learning and NLP, it didn't seem like the most impactful use of my time, or the most value I could contribute through my research. 

We took a step back, and considered where I might add value to the system. We discussed the fact that these models, despite being significant advancements and impressive research, were still nowhere near consistently challenging a human writer, and that curation of the AI productions was still required. This seemed like an area for exploration, to create an environment for human writers to work together with a language generation model, editing and inserting their own text as they go. A decision was made to pursue this direction.

\subsection{Implementation}

The implementation of GPT-2 I settled on was \href{https://huggingface.co/}{Huggingface}'s (https://huggingface.co/) repository of \href{https://huggingface.co/transformers/}{Transformers} (https://huggingface.co/transformers/). This contains not only GPT-2, but a collection of all state-of-the-art architectures, which leaves room for extensibility and including other models in the future.

The code provided in this repository allows for a variety of approaches for optimal accessibility: from simply running generation on a pre-trained model, to fine-tuning your own model, to custom generation or even altering the lower level code. A variety of scripts were included, making it quick and straightforward to get up and running. This seemed ideal for our purposes.

\subsection{Training}

One concern that had prevailed since we delved into NNs was the hardware that is required to train these models. Transformers utilise Convolutional Neural Networks, which are faster than the Recurrent variant but still require significant computing power. Throughout the development process there was limited availability of, and accessibility to, hardware, which are especially important for the training phase.

Fortunately, alternatives do exist, and there are numerous cloud services available for training machine learning models on third-party hardware: \href{https://www.kaggle.com/}{Kaggle}, \href{https://lambdalabs.com/}{Lambda} and \href{https://www.wandb.com/}{Weights \& Biases}, just to name a few. There were drawbacks, namely cost, but these were promising.

Having analysed all the available alternatives a decision was made to proceed with \href{https://colab.research.google.com/}{Google Colab} (https://colab.research.google.com/), which provides a Python Notebook interface and allows you to run code on advanced (NVIDIA Tesla K80) GPUs. There are limits on time and use cases, but I was comfortable I could fall within or work around these restrictions, especially considering that the product was free to use. Of course, if we wanted to train a model then we would need data.

\subsection{Data}

Another issue that came up in the early stages was scope. Often an issue with projects of this nature, we wanted to make sure that we had a reasonable prototype which demonstrated progress. To generalise from the beginning would make the model's ability to optimise less clear, so we decided to focus on one type of story initially and perhaps generalise later, or at least leave that option open to ourselves.

I was careful when choosing a genre, since it was important to have a large and high quality data source to support it. Being a \href{https://www.reddit.com/}{Reddit} (https://www.reddit.com/) user for some time, I recognised the wealth of data available on the website, not just as a whole but categorised neatly into different sub-communities dedicated to certain topics. 

The \href{https://www.reddit.com/r/writingprompts}{Writing Prompts} (https://www.reddit.com/r/writingprompts) subreddit was the leading contender. A forum devoted to short story writing, it seemed like the perfect match. However, it was not specific to any genre of story and categorisation of the stories was minimal, meaning it was more of a general data source. Good, but not quite what we needed.

having carefully analysed the alternatives, a decision was made to use \href{https://www.reddit.com/r/nosleep}{No Sleep} (https://www.reddit.com/r/nosleep), a community for short horror stories which, as the title goes, denied their users sleep. This was much more suitable, with focused data and a significant amount of it too. As of 26 March 2020, the subreddit has been going for 10 years with 13.9 million subscribers. This was an ideal data source for our purposes.


\section{User Experience}

Simple webapp, user interaction, generate and change parameters, add/edit text as you go (Sketches of UI and structure)

\subsection{Back End}

Python/Flask API to call scripts based on huggingface implementation, take in parameters and text, return json with text object of generated string to replace existing one

\subsection{Front End}

ReactJS framework, library for building UI. Make calls to API and display results.



%\subsection{Creating a Table}
%
%\begin{table}[htp]
%\caption{Table Title}
%\begin{center}
%\begin{tabular}{| p{3cm} | c | c | c |}
%\hline 
% & Resolution & Min & Max \\ \hline 
%Gyroscope & 4.5mV / {$^{\circ}$}/s & 0.27 $^{\circ}$/s & 406 $^{\circ}$/s \\ \hline
%Accelerometer & 600mV /g &  0.002g & 2g \\ \hline
%Magnetometer & 385mV/ gauss & 0.317 gauss & 6 gauss \\ \hline
%\end{tabular}
%\end{center}
%\label{Sensors' Resolution}
%\end{table}%


%% here is how to do a page break
%\pagebreak
%%done
%
%\section{Quote}
% An in-text quote is declared in the following way:
%
%\begin{quote}
%
%Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward!    (CommonSense, p. 0) 
%
% \end{quote}
%
%Another way of doing it is:
%\begin{quote}
%\centering
%\emph{There are in our existence spots of time, \\
%That with distinct pre-eminence retain \\
%A renovating virtue, whence-depressed \\
%By false opinion and contentious thought, \\
%Or aught of heavier or more deadly weight, \\
%In trivial occupations, and the round \\
%Of ordinary intercourse-our minds \\ 
%Are nourished and invisibly repaired; \\ 
%A virtue, by which pleasure is enhanced, \\ 
%That penetrates, enables us to mount, \\ 
%When high, more high, and lifts us up when fallen. \\ 
%}
%\vspace{3mm}
%\raggedleft (\Citealt[verses 208-218]{poem})
%\end{quote}


%\section{Creating a list}
%
%\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
%\item \textbf{(2000)} item 1. 
%\item \textbf{(2004)} item 2. 
%\item \textbf{(2010)} item 3. 
%\item \textbf{(2013)} item 4. 
%\end{itemize}





% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------