% this file is called up by thesis.tex
% content in this file will be fed into the main document



%: ----------------------- name of chapter  -------------------------
\chapter{Evaluation} % top level followed by section, subsection

\ifpdf
    \graphicspath{{5_chapter5/figures/PNG/}{5_chapter5/figures/PDF/}{5_chapter5/figures/}}
\else
    \graphicspath{{5_chapter5/figures/EPS/}{5_chapter5/figures/}}
\fi

Evaluation of the system's productions was difficult for a number of reasons. Metrics available for language generation are limited, largely focused on classification (GLUE (\url{https://gluebenchmark.com}), question answering (SQuAD (\url{https://rajpurkar.github.io/SQuAD-explorer/}), or selection of the correct output from a set of options (ROCStories (\url{https://cs.rochester.edu/nlp/rocstories/}). These all relate to Natural Language Understanding, rather than Generation, and ultimately the gold standard in evaluating AI generated text is still human evaluation. 

Furthermore, since our system is intended to be used in collaboration with a human writer, pure metrics of the generation in isolation are not necessarily indicative of its efficacy. Human writers could easily correct small mistakes and vastly improve the quality of the story, but it is reasonable to suggest that we should still optimise the model to produce high quality and interesting text. It would be better to require less modification by the human.

\section{Perplexity} \label{perplexity}

We attempted to utilise some metrics, namely Perplexity (Section~\ref{perplexity}), at the very least to establish a baseline versus other models, particularly the non-fine-tuned GPT-2 from which we built our custom model on horror stories.

Perplexity is a straightforward measure for predicting how well a model can replicate validation data, from given training data (\Citealt{jelinek1977perplexity}). For our purposes, we split our initial datasets into training and validation sets (with an 80:20 split). In this case, a lower score is better, indicating the model is better at replicating stories. 

\begin{table}[ht]
\centering
\begin{tabular}[t]{lr}
\toprule
Model &Perplexity\\
\midrule
Top 1000 posts (\mytilde10MB)&19.44\\
Posts over 400 points (\mytilde100MB)&21.12\\
Default GPT2-small&37.50\footnotemark\\
\bottomrule
\end{tabular}
\caption{Perplexity Scores of Datasets and Control}
\label{perplexity_table}
\end{table}%
\footnotetext{Source: (\Citealt{radford2019language})}

As we can see in Table~\ref{perplexity_table}, both of our datasets produced models with far better perplexity than the default GPT-2 small model. This is to be expected, since it is an earlier version of GPT-2 and trained on a much more generic dataset. With a broader range of text, prediction is naturally more difficult. One concern was our larger dataset actually received a slightly worse perplexity score than the original, smaller set. This required some consideration.

We reasoned that taking a narrower slice of stories, the top 1000 posts of the NoSleep subreddit, they seemed to have more in common. Thus, prediction was easier, since the stories were more similar. This had interesting implications, suggesting that our original research into stories (Section~\ref{stories}) had correctly surmised that there are common elements to good stories. At least, among popular ones.

However, we felt that despite the marginal loss in complexity, the tenfold gain in data from which to draw on made the larger dataset the preferable option to proceed with. It was important for our system to have some degree of variety and creativity, even within the bounds of the horror genre, and having access to more data would aid this. We decided not to proceed further with larger datasets, owing to large computation costs and fear of tipping the balance, ending up with significantly worse complexity.


\section{Human Review}

While metrics can provide some reliable feedback on the pure quality of language, human evaluation is ultimately still necessary for evaluating the quality of the actual content (\Citealt{reiter2009investigation}). That is, while generated text may be syntactically correct or follow similar patterns as training data, that is not to say it actually produced a high quality, entertaining or flowing story. 

Human review has the obvious downside of being rather slow, but in the case of our product, this is part of the writing process. Humans are supposed to be reviewing the AI productions in real time and altering those to their needs. In this way, it feels like a natural evaluation of the system.

\subsection{Example Story}

To analyse a sample story, we will reference Appendix~\hyperref[appendix:story]{B}: "Voices". The system was only provided the generic starting phrase "Once upon a time", and left to generate a short story autonomously. We believe this exhibits many of the factors that make up a good story, touched on in Section~\ref{stories}.

\begin{itemize}
\item A setting and mood is established from the beginning: a haunted building, a room that can not be left, demons, nightmares. This is maintained throughout the story.
\item Logical series of events and locations: "I could hear the voices coming from downstairs. My bedroom. [...] I was going to go back down to the basement and try and sleep [...] I didn't sleep. I didn't dream." 
\item Characters introduced and remembered: "The sound of my mom crying [...] My mom said that she didn't believe me [...] my mom found me in my room [...] She asked me". Note that pronouns are also used where relevant, continuing the actions of one character. However, the character is appropriately named again after some time has passed.

\subsection{Anonymous Reviews}

In order to obtain some additional, outside feedback, we posted one of the generated stories (with slight modification) back to the NoSleep subreddit from which we had trained our model. There are many random factors that contribute to the popularity of a reddit post, but we reckoned that even a few points would be some kind of success. 

The story posted can be found in Appendix~\hyperref[appendix:story2mod]{D}, modified slightly from the original generation in Appendix~\hyperref[appendix:story2]{C}. The post performed surprisingly well\footnote{\href{https://www.reddit.com/r/nosleep/comments/gaccvf/lonely/}{Reddit Post} (\url{https://www.reddit.com/r/nosleep/comments/gaccvf/lonely/})}, garnering 5 points and a 100\% approval rate at the time of writing, but this was not an entirely accurate test of the system. It is supposed to be a much more collaborative effort, so minor faults are acceptable in the name of inspiration. Still, this proved that the system had some degree of effectiveness, even when left largely to its own devices.



% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

