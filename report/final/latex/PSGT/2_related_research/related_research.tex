% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- name of chapter  -------------------------

\chapter{Related Research} % top level followed by section, subsection

%: ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{2_related_research/figures/PNG/}{2_related_research/figures/PDF/}{2_related_research/figures/}}
\else
    \graphicspath{{2_related_research/figures/EPS/}{2_related_research/figures/}}
\fi

%: ----------------------- contents from here ------------------------

Much research has been undertaken on this topic, with a variety of approaches.


\section{Stories}

It is important for us to understand the most basic, linguistic concepts before we move on to algorithmic productions. What are the elements that make up a good story? What constitutes a good story? What even is a story? 

According to Webster (\Citealt{dictionary2002merriam}), a \emph{Story} is: 
\begin{quote}
An account of incidents or events, [either] regarding the facts pertinent to a situation in question, [or] a fictional narrative.
\end{quote}

With a \emph{narrative} being:
\begin{quote}
A way of presenting or understanding a situation or series of events that reflects and promotes a particular point of view or set of values.
\end{quote}

This is naturally a broad set of definitions, but it does give us some cues. From a story, we would expect a series of events related to each other in some way, either to describe a situation or promote a certain worldview or message. That is, there is an overall connection and cohesiveness to the piece. Literary critics have posited several interpretations or generalisations: that stories begin in equilibrium before being disrupted, and ultimately involve a journey back to equilibrium (\Citealt{todorov1969structural}), but yet more argue that there can be no "correct" definition determined (\Citealt{sullivan2002reception}). The fields of Literary Theory and Narratology emerged in an attempt to dissect and formulate stories, which we will discuss more later.

I first set out to examine commonly used techniques used in various aspects of storytelling. These will perhaps be more relevant in evaluating the productions of my system, rather than guiding them too much, depending on the level of autonomy, and the rigidity of training and generation it has.

\subsection{Structure}

Structure describes the underlying framework of a story and, as the highest level of a story planning process, was first to be investigated. 

The classic structure would be the three acts. This dates back to Aristotle in 400BC, describing a story as having three parts: a beginning, an end and a middle (\Citealt{mack1980norton}), and is still popular today. It is commonly depicted something like (Figure \ref{3-act-structure}).
 
\figuremacroW{3-act-structure}{Three Act Structure}{Breakdown of three act structure. [Source: (\Citealt{3-act-structure-image})]}{0.8}

Stories are broken town into distinct sections: setup and exposition, rising action and confrontation, climax and resolution (\Citealt{trottier1998screenwriter}). This is a bit more versatile, which is something to consider for the later steps of “filling in the gaps” with our algorithmic approach. A balance must be struck between: providing some structure so that the generation has some level of cogency, but also allowing flexibility so that not all stories are the same.

Other popular methods include the Hero’s Journey (\Citealt{campbell2008hero}), which describes a cycle of sorts: a call to adventure, crossing from the known to the unknown, transformation etc. This is a more granular structure, most known for its application in Star Wars. See (Figure \ref{heros-journey}).
 
\figuremacroW{heros-journey}{The Hero's Journey}{A cycle of the hero's journey. [Source: (\Citealt{vogler1985practical})]}{0.8}

However, that is not to say these are the only structures that must be followed, nor that there must be that traditional arc. Some authors have examined spiral, fractal and explosive patterns in literature (\Citealt{alison2019meander}), rejecting the historical, structural norms. It is tempting to declare adherence to a structure irrelevant, but patterns do remain, so this cannot be ignored entirely.

\subsection{Plot}
\label{story_plot}

Delving deeper into the story, plot makes up those events which are significant, have consequences and make a difference to the story (\Citealt{dibell1999elements}). If we examine our three act structure figure from earlier (Figure \ref{3-act-structure}), we see ticks along the line of the story, larger incidents which have an impact.

Indeed, a scene or series of events may be memorable and iconic, but if they do not serve as major events that progress the overall narrative, then they do not constitute plot (\Citealt{alcorn2014know}). Following on from the three-act structure, plot points would be used to connect the acts to each other, for example: our protagonist is thrust into an unexpected situation, they face a setback and it seems all hope is lost, finally they overcome.

For the context of our system, plot points should serve as transitional pieces, advancing the story in some way so we don't simply remain at or revert back to the previous content. This must be handled with care, as too few plot points would be boring, but too many would be bewildering.

\subsection{Setting}

This refers to the time, location and milieu in which the story occurs (\Citealt{lodge2012art}) often referred to as the "world" or "universe", in modern works. 

This serves as the backdrop of our story, and to feel authentic it must be rich with context and history. At their best, settings are so specific that they provide natural associations to the reader (\Citealt{kuntz1993narrative}), setting the mood and plot anticipations.

When generating content, there should be at minimum a consistency of setting, and ideally it should have enough detail to establish a mood that carries through the story.


\section{Narratology}

Having touched on literary theories, I was set on to the work of Vladimir Propp on Narratology (the study of narrative) and his early work in formulating elements of stories. Specifically, his seminal work with Morphology of the Folktale (\Citealt{propp1968morphology}), originally written in 1928. He, along with other Russian formalists, took a modern approach to narratology after Aristotle's ancient theorising.

They distinguished the syuzhet (plot) from the fabula (story). The idea was that the story is the raw material, familiar in many ways already, and it is \emph{defamiliarised} (a term they coined) into the plot, a new organisation and the way the story is told. This goes back to our previous point on Plot (\ref{story_plot}), which pointed out that plot pertains to the information that pushes a story along. They are subtly different concepts.

\subsection{Abstractions}

Propp's work is very relevant to this research, since he has some of the earliest work on abstracting and formulating aspects of stories (specifically Russian folktales).

He first curated a table of possible events at various stages of a story, then associated these with symbols and combined them with functions into what look like mathematical formulas. An example looks something like: (Figure \ref{propp-eg}).
 
\figuremacroW{propp-eg}{Morphology of the Folktale}{Breaking down a story. [Source: (\Citealt{propp1968morphology})]}{0.8}

There were hundreds of these, assembled in a tabular format like so: (Figure \ref{propp-eg-2}).
 
\figuremacroW{propp-eg-2}{Morphology of the Folktale}{Table of stories. [Source: (\Citealt{propp1968morphology})]}{0.8}

This laid groundwork for the development of grammars, while this is perhaps too rigid and systemic to be applicable, as argued by some (\Citealt{dundes1997binary}). 


\section{Formal Language Theory}

(Formal) Grammars were devised as a more generic and granular approach to generating strings of text, by following certain rules, from a certain alphabet (\Citealt{reghizzi2013formal}). Compared to Propp's work on formulating \emph{elements}, this was work being done down to the character level, getting closer to what we would need for algorithmic generation.

Emil Post was one of the early innovators in this area, creating the Post Canonical System in 1943, a string manipulation system for generating instances of a language, from an initial alphabet and rules (\Citealt{post1943formal}). 

\subsection{Grammars}

Noam Chomsky then proposed a set of generative grammars in 1956, classified in the \emph{Chomsky Hierarchy} (\Citealt{chomsky1956three}), with different levels of strictness in their rules. The two efficient and popular types were the Context Free Grammar and Regular Grammar.

Chomsky grammars consist of a finite set of production rules (left-hand side$\,\to\,$right-hand side), where each side consists of a finite sequence of the following symbols:
\begin{itemize}
\item a finite set of nonterminal symbols (indicating a production rule can be applied)
\item a finite set of terminal symbols (indicating no production rule can be applied)
\item a start symbol (a distinguished nonterminal symbol that is not found on any right hand side, and so cannot be produced anyway else)
\end{itemize}

For example:

\begin{equation}
    S \rightarrow AB
\end{equation}\begin{equation}
    S \rightarrow \lambda (empty string)
\end{equation}\begin{equation}
    A \rightarrow aS
\end{equation}\begin{equation}
    B \rightarrow b
\end{equation}

This is a Context Free Grammar (CFG) that could generate a string of letters "a" and "b". 

Computing systems for grammar and story generation have been built (\Citealt{compton2014tracery}, )however, formal grammars like this require significant human building and labelling. While these are interesting and worth exploring from a historical perspective, they are troubling from the perspectives of extensibility and originality.


\section{Natural Language Generation}

A subfield of linguistics and computer science, Natural Language Processing (NLP) deals with making easier the interaction between humans and computers, enabling machines to more easily understand natural, human language. It has risen to prominence since the 1990s in line with the rise of machine learning as a programming technique (\Citealt{johnson2009statistical}). Before this, early language processing systems were based on handwritten rules like the grammars outlined above (\Citealt{schank2013scripts}), which meant a comparative lack of knowledge, or features – weights on different choices based on the data. This machine learning approach allowed programs to learn rules by themselves, via analysing large amounts of input data.

Further developments in the 2010s brought the advancement of deep learning and neural networks, which could achieve better results than ever before (\Citealt{goldberg2016primer}). We will examine these in more detail shortly, as these results made them quickly became the leading contender for our system.

It is worth noting that the NLP field is wide, and for our purposes we will primarily focus on the area of Natural Language Generation (NLG). Natural Language Understanding, which we are perhaps more familiar with in things like virtual assistants, aims to take in natural language, abstract it and produce some kind of representation of the idea being conveyed (\Citealt{reiter2000building}). Conversely, NLG attempts to take what is structured data, weights and biases based on input data, and produce natural language.

\subsection{Neural Networks}

An Artificial Neural Network (ANN) is a computing system that seeks to emulate the kind of processing and problem solving done by the human brain, designed around pattern recognition (\Citealt{haykin1994neural}). The core principle is that they “learn” to perform said task by analysing examples and figuring out their own rules, rather than being explicitly told any rules up front. 

An ANN is modelled after the human brain, containing nodes, or "neurons", which are connected to each other. They can receive input, process this with their internal state, and produce output (\Citealt{winston1992artificial}). This can be passed as a message to another neuron or ultimately, complete the task at hand. A basic example of the architecture looks something like the following: (Figure \ref{nn-sample}).
 
\figuremacroW{nn-sample}{Artificial Neural Network Architecture}{Neurons and connections. [Source: (\Citealt{nn-sample-img})]}{0.6}

Each connection between neurons has an associated weight, which represents its relative importance, which is taken into account by the receiving neuron processing its inputs.

The classic example and popular use case is image identification (\Citealt{le2013building}). To teach a program to correctly separate images of cats and dogs, the basic approach would be to give it explicit rules for cats versus dogs – nose, ears, paws, etc. However, the ANN allows you to simply feed it examples (ideally many) of images of both, and then allow it to formulate those rules by itself. This saves time and work for the user, and tends to create a much more accurate model (\Citealt{lecun1989backpropagation}), especially with a large amount of input, "training" data.


\subsubsection{Recurrent Neural Networks \& LSTMs}

Examining some of the recent works on natural language generation and even specific story generation papers, Recurrent Neural Networks (RNNs) stood out as the popular technology (\Citealt{peng2018towards}), (\Citealt{fan2018hierarchical}), (\Citealt{sutskever2014sequence}). 

These are like neural networks, but with loops that provide a kind of memory or persistence (\Citealt{graves2008novel}). These have feedback connections and can process sequences of data, instead of just single data points like a typical (feedforward) neural network. Particularly Long Short Term Memory (LSTM) networks, which are better at remembering long term dependencies. See (Figure \ref{rnn-unrolled}).
 
\figuremacroW{rnn-unrolled}{Recurrent Neuron}{Loops for persistence [Source: (\Citealt{olah2015rnn})]}{0.8}

This is incredibly important for NLG, because context matters. When identifying images, as with the previous example, this isn’t such a big deal. Examples are distinct and don’t depend on each other – one input, one output. However, with video or language, you must generate the next part of your output, piece by piece. In this case, remembering what was written previously is vital, not only to make coherent sentences but also to construct an overall narrative throughout the story. Characters should develop, plot points should advance, relevant twists should occur, and so on.


\subsection{Transformers}

However, these RNNs and LSTMs appear not to be state of the art in the NLP world anymore. Things move quickly, and that has led us to the Transformer architecture. 

The idea was pioneered by a team of Google researchers (\Citealt{vaswani2017attention}) and the early results are very promising. OpenAI’s GPT-2 (\Citealt{radford2019language}) is perhaps the most advanced language processing Artificial Intelligence (AI) system currently, so much so that they declined to release their full code, claiming they fear it could be used for ill means e.g. fake news.

We mentioned RNNs utilising loops, but in reality it’s more like a series of connected Neurons, each performing its processing and then passing the new “state” to the next. This is what allows it to maintain memory – each stage of the computation is aware of everything that happened previously. You can see this in the following, with the RNN on top and Transformer on bottom: (Figure \ref{attention}).
 
\figuremacroW{attention}{RNN vs. Transformer Architecture}{Attention replacing loops [Source: (\Citealt{kurita2017attention})]}{0.8}

Note: here the example is translation, but for language generation the idea is the same. Instead of passing in a passage and translating it, we seek to find the next part of said passage. Note also that there are encoding (orange) and decoding (blue) stages, but they are practically similar. 

The issues here are clear. The RNN is very difficult to parallelise since the computation must be done in sequence, passing the state along (\Citealt{bengio1994learning}). Also, the further you drift from the start (i.e. the longer the text becomes) the less your network will remember about those early stages. This is a problem for stories, where elements are often mentioned briefly at the start, only to come into prominence later – vital for continuity.

Transformers, by contrast, take the entire input passage in at once, using the concept of Attention (\Citealt{vaswani2017attention}) along with positional encoding (to ensure you don’t start mixing up a sentence) to calculate the relative importance of each word. These results are then passed through a more traditional feedforward Neural Network (much faster than an RNN) and potentially another multi-head attention stage (applications differ) to produce your following piece of the passage. 

For the purposes of this research, we won’t be delving into the inner workings of the Attention mechanism, but essentially what it does is calculate the relevance of each word in the input. For example, if you have the input “I grew up in France… I speak fluent” and wish to predict the next word, the word “France” will be given a much higher score, indicating to the system what it should predict based on.

This helps hugely with the long term dependency issue, since at each stage the entire input is considered, and thus there is a much lower risk of earlier information being “forgotten”, something that even the finest LSTMs struggled with. 

This could lead to performance issues, but thanks to the Attention mechanism and feedforward NN being used (allowing you to process the entire input at once), Transformers are in fact faster.

The best models also seem to utilise unsupervised learning (to be discussed later), which has the obvious benefit of being able to feed it much more data, as seen with GPT-2 and its 1.5 billion parameters (\Citealt{radford2019language}). However, you do need to feed it a massive amount of data for it to be in any way effective. 


\subsubsection{BERT}

Google

\subsubsection{GPT}

First attempt

\subsubsection{GPT-2}

New and improved!



% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

