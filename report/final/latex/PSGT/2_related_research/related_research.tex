% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- name of chapter  -------------------------

\chapter{Related Research} % top level followed by section, subsection

%: ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{2_related_research/figures/PNG/}{2_related_research/figures/PDF/}{2_related_research/figures/}}
\else
    \graphicspath{{2_related_research/figures/EPS/}{2_related_research/figures/}}
\fi

%: ----------------------- contents from here ------------------------


\section{Stories} \label{stories}

It is important to understand the most basic, linguistic concepts before we begin any investigation into algorithmic productions. What are the elements that make up a good story? What even is a story? 

According to Webster (\Citealt{dictionary2002merriam}), a \emph{Story} is: 
\begin{quote}
An account of incidents or events, [either] regarding the facts pertinent to a situation in question, [or] a fictional narrative.
\end{quote}

With a \emph{narrative} being:
\begin{quote}
A way of presenting or understanding a situation or series of events that reflects and promotes a particular point of view or set of values.
\end{quote}

This is naturally a broad set of definitions, but it does provide us with a good starting point and indicate areas that need to be investigated further. From a story, we would expect a series of events related to each other in some way, either to describe a situation or promote a certain worldview or message. That is, there is an overall connection and cohesiveness to the piece. Literary critics have posited several interpretations or generalisations: that stories begin in equilibrium before being disrupted, and ultimately involve a journey back to equilibrium (\Citealt{todorov1969structural}), but yet more argue that there can be no "correct" definition determined (\Citealt{sullivan2002reception}). The fields of Literary Theory and Narratology emerged in an attempt to dissect and formulate stories, which we will discuss more in Section~\ref{narratology}.

We first set out to examine commonly used techniques in various aspects of storytelling. These will perhaps be more relevant in evaluating the productions of our system, rather than guiding the production itself too much, depending on the level of autonomy, and the rigidity of training and generation it has.

\subsection{Structure}

Structure describes the underlying framework of a story and, as the highest level of a story planning process, was first to be investigated. The classic structure would be the three acts. This dates back to Aristotle in 400BC, describing a story as having three parts: a beginning, an end and a middle (\Citealt{mack1980norton}), and is still popular today. It is commonly depicted something like (Figure~\ref{3-act-structure}).
 
\figuremacroW{3-act-structure}{Three Act Structure}{Breakdown of three act structure. [Source: (\Citealt{3-act-structure-image})]}{0.8}

Stories are broken town into distinct sections: setup and exposition, rising action and confrontation, climax and resolution (\Citealt{trottier1998screenwriter}). This is a bit more versatile, which is something to consider for the later steps of “filling in the gaps” with our algorithmic approach. In the context of PCG, a balance must be struck between: providing some structure so that the generation has some level of cogency, but also allowing flexibility so that not all stories are the same.

Other popular structures include the Hero’s Journey (\Citealt{campbell2008hero}), which describes a cycle: a call to adventure, crossing from the known to the unknown, transformation etc. This is a more granular structure, most known for its application in Star Wars (\Citealt{gordon1978star}). See (Figure \ref{heros-journey}).
 
\figuremacroW{heros-journey}{The Hero's Journey}{A cycle of the hero's journey. [Source: (\Citealt{vogler1985practical})]}{0.8}

However, that is not to say these are the only structures that must be followed, nor that there must be that traditional arc. Some authors have examined spiral, fractal and explosive patterns in literature (\Citealt{alison2019meander}), rejecting the historical, structural norms. It is tempting to declare adherence to a structure irrelevant, but patterns do remain, so this cannot be ignored entirely.

\subsection{Plot}
\label{story_plot}

Delving deeper into the story, plot makes up those events which are significant, have consequences and make a difference to the story (\Citealt{dibell1999elements}). If we examine our three act structure figure from earlier (Figure \ref{3-act-structure}), we see ticks along the line of the story, larger incidents which have an impact.

Indeed, a scene or series of events may be memorable and iconic, but if they do not serve as major events that progress the overall narrative, then they do not constitute plot (\Citealt{alcorn2014know}). Following on from the three-act structure, plot points would be used to connect the acts to each other, for example: our protagonist is thrust into an unexpected situation, they face a setback and it seems all hope is lost, finally they overcome.

For the context of our system, plot points should serve as transitional pieces, advancing the story in some way so we don't simply remain at or revert back to the previous content. This must be handled with care, as too few plot points would be boring, but too many would be bewildering.

\subsection{Setting}

This refers to the time, location and milieu in which the story occurs (\Citealt{lodge2012art}) often referred to as the "world" or "universe", in modern works. This serves as the backdrop of our story, and to feel authentic it must be rich with context and history. At their best, settings are so specific that they provide natural associations to the reader (\Citealt{kuntz1993narrative}), setting the mood and plot anticipations.

When generating content, there should be at minimum a consistency of setting, and ideally it should have enough detail to establish a mood that carries through the story.


\section{Narratology} \label{narratology}

Having touched on literary theories, this led on to the topic of narratology (the study of narrative). Specifically, the work of Vladimir Propp and his early efforts in formulating elements of stories, most notably with his seminal work: Morphology of the Folktale (\Citealt{propp1968morphology}), originally written in 1928 (originally in Russian, hence the 1968 translation is referenced). He, along with other Russian formalists, took a modern approach to narratology after Aristotle's ancient theorising.

They distinguished the syuzhet (plot) from the fabula (story). The idea was that the story is the raw material, familiar in many ways already, and it is \emph{defamiliarised} (a term they coined) into the plot, a new organisation and the way the story is told. This goes back to our previous point on Plot, Section~\ref{story_plot}, which pointed out that plot pertains to the information that pushes a story along. They are subtly different concepts.

\subsection{Abstractions}

Propp's work is very relevant to this research, since he has some of the earliest work on abstracting and formulating aspects of stories (specifically Russian folktales).

He first curated a table of possible events at various stages of a story, then associated these with symbols and combined them with functions into what look like mathematical formulas. An example looks something like: (Figure~\ref{propp-eg}). Here you can see Propp break down the elements of the stories, like characters and actions, codified into symbols. These are then combined into a formula for the story as a whole.
 
\figuremacroW{propp-eg}{Morphology of the Folktale}{Breaking down a story. [Source: (\Citealt{propp1968morphology})]}{0.8}

There were hundreds of these, assembled in a tabular format like so: (Figure~\ref{propp-eg-2}). Each row is a different type of story, and the columns are filled in as appropriate for each element the story contains. 
 
\figuremacroW{propp-eg-2}{Morphology of the Folktale}{Table of stories. [Source: (\Citealt{propp1968morphology})]}{0.8}

While this is perhaps too rigid and systemic to be applicable in any broad sense, as argued by some (\Citealt{dundes1997binary}), it bears some resemblance to Grammars which were developed later, also worth investigation. 

\section{Formal Language Theory}

Grammars were devised as a more generic and granular approach to generating strings of text, by following certain rules, from a certain alphabet (\Citealt{reghizzi2013formal}). Compared to Propp's focus on formulating \emph{elements}, this work narrowed down to the character level, getting closer to what we would need for algorithmic generation.

Emil Post was one of the early innovators in this area, creating the Post Canonical System in 1943, a string manipulation system for generating instances of a language, from an initial alphabet and rules (\Citealt{post1943formal}). 

\subsection{Grammars} \label{grammars}

Noam Chomsky proposed a set of generative grammars in 1956, classified in the \emph{Chomsky Hierarchy} (\Citealt{chomsky1956three}), with different levels of strictness in their rules. The two efficient and popular types were the Context Free Grammar and Regular Grammar.

Chomsky grammars consist of a finite set of production rules in the form of: (left-hand side$\,\to\,$right-hand side), where each side consists of a finite sequence of the following symbols:
\begin{itemize}
\item a finite set of nonterminal symbols. With a nonterminal symbol indicating a production rule can be applied
\item a finite set of terminal symbols. With a terminal symbol indicating no production rule can be applied
\item a start symbol. With a start symbol being a distinguished nonterminal symbol that is not found on any right hand side, and so cannot be produced anyway else
\end{itemize}

For example, Equation~\ref{cfg_eg} is a Context Free Grammar (CFG) that could generate a string of letters "a" and "b". 

\begin{equation} \label{cfg_eg}
\begin{split}
&S \rightarrow AB \\
&S \rightarrow \lambda \\
&A \rightarrow aS \\
&B \rightarrow b
\end{split}
\end{equation}

Computing systems for grammar and story generation have been built (\Citealt{compton2014tracery}), however, formal grammars like this require significant human building and labelling. While these are interesting and worth exploring, they are troubling from the perspectives of extensibility and originality when it comes to our procedural generation prototype.


\section{Natural Language Generation}

A subfield of linguistics and computer science, Natural Language Processing (NLP) deals with making more fluid the interaction between humans and computers, enabling machines to more easily understand natural, human language. It has risen to prominence since the 1990s in line with the rise of machine learning as a programming technique (\Citealt{johnson2009statistical}). Before this, early language processing systems were based on handwritten rules like the grammars outlined in Section~\ref{grammars} (\Citealt{schank2013scripts}), which meant a comparative lack of knowledge, or features – weights on different choices based on the data. The machine learning approach allowed programs to learn rules by themselves, via analysing large amounts of input data.

Further developments in the 2010s brought the advancement of deep learning and neural networks, which could achieve better results than ever before (\Citealt{goldberg2016primer}). We will examine these in more detail in Section~\ref{neuralnets}, as the quality of the results these systems were able to achieve, along with the abundance of solutions, made them very promising techniques to incorporate into our system

It is worth noting that the NLP field is wide, and for our purposes we will primarily focus on the area of Natural Language Generation (NLG). Natural Language Understanding, which we are perhaps more familiar with in things like virtual assistants, aims to take in natural language, abstract it and produce some kind of representation of the idea being conveyed (\Citealt{reiter2000building}). Conversely, NLG attempts to take what is structured data, weights and biases based on input data, and produce natural language.

\subsection{Neural Networks} \label{neuralnets}

An Artificial Neural Network (ANN) is a computing system that seeks to emulate the kind of processing and problem solving done by the human brain, designed around pattern recognition (\Citealt{haykin1994neural}). The core principle is that they “learn” to perform this pattern recognition by analysing examples and figuring out their own rules, rather than being explicitly told any rules up front. 

An ANN contains nodes, or "neurons", which are connected to each other. They can receive input, process this with their internal state, and produce output (\Citealt{winston1992artificial}). This can be passed as a message to another neuron or ultimately, complete the task at hand. A basic example of the architecture looks something like the following: (Figure \ref{nn-sample}). Each connection between neurons has an associated weight, which represents its relative importance, which is taken into account by the receiving neuron processing its inputs.
 
\figuremacroW{nn-sample}{Artificial Neural Network Architecture}{Neurons and connections. [Source: (\Citealt{nn-sample-img})]}{0.6}

The classic example and popular use case of ANNs is image identification (\Citealt{le2013building}). To teach a program to correctly separate images of cats and dogs, the basic approach would be to give it explicit rules for cats versus dogs – nose, ears, paws, etc. However, the ANN allows you to simply feed it examples (ideally many) of images of both, and then allow it to formulate those rules by itself. This saves time and work for the user, and tends to create a much more accurate model (\Citealt{lecun1989backpropagation}), especially with a large amount of input, "training" data. Moreover, in some cases (like ours) it is not feasible to create a comprehensive list of rules by hand, and there is often an abundant source of data to feed the model.


\subsubsection{Recurrent Neural Networks \& LSTMs} \label{rnn}

Examining some of the recent works on natural language generation and even specific story generation papers, Recurrent Neural Networks (RNNs) stood out as the popular technology (\Citealt{peng2018towards}), (\Citealt{fan2018hierarchical}), (\Citealt{sutskever2014sequence}). 

These are like neural networks, but with loops that provide a kind of memory or persistence (\Citealt{graves2008novel}). These have feedback connections and can process sequences of data, instead of just single data points like a typical (feedforward) neural network. Particularly Long Short Term Memory (LSTM) networks, which are better at remembering long term dependencies, as exhibited in (Figure~\ref{rnn-unrolled}).
 
\figuremacroW{rnn-unrolled}{Recurrent Neuron}{Loops for persistence [Source: (\Citealt{olah2015rnn})]}{0.8}

Long term dependency is incredibly important for NLG, because context matters. When identifying images, as with the previous example, this isn’t such a big deal. Examples are distinct and don’t depend on each other – one input, one output. However, with video or language, you must generate the next part of your output, piece by piece. For the purposes of our system, it is important to realise that a story has a sequence, not only to make coherent sentences but also to construct an overall narrative throughout the story. Characters should develop, plot points should advance, relevant twists should occur, and so on.

However, these RNNs and LSTMs appear not to be state of the art in the NLP world anymore. Technologies have developed quickly, and that has led us to the Transformer architecture. 


\subsection{Transformers}

The Transformer architecture was pioneered by a team of Google researchers (\Citealt{vaswani2017attention}) and the early results are very promising. OpenAI’s GPT-2 (\Citealt{radford2019language}) is perhaps the most advanced language processing Artificial Intelligence (AI) system currently (discussed more in Section~\ref{gpt}), so much so that they declined to release their full code, claiming they fear it could be used for ill means e.g. fake news.

We mentioned RNNs utilising loops in Section~\ref{rnn}, but in reality it’s more like a series of connected Neurons, each performing its processing and then passing the new “state” to the next. This is what allows it to maintain memory – each stage of the computation is aware of everything that happened previously. You can see this in Figure~\ref{attention}, with the RNN on top and Transformer on bottom.
 
\figuremacroW{attention}{RNN vs. Transformer Architecture}{Attention replacing loops [Source: (\Citealt{kurita2017attention})]}{0.8}

It should be noted that in the Figure~\ref{attention} example the targeted operation is translation, but for language generation the idea is the same. Instead of passing in a passage and translating it, we seek to find the next part of said passage. Note also that there are encoding (orange) and decoding (blue) stages, but they are practically similar. 

The issues here are clear: the RNN is very difficult to parallelise since the computation must be done in sequence, passing the state along (\Citealt{bengio1994learning}); also, the longer the generation runs (i.e. the longer the text becomes) the less your network will remember about those early stages. This could potentially pose problems if it was used in the generation of stories, where elements are often mentioned briefly at the start, only to come into prominence later – vital for continuity.

Transformers, by contrast, take the entire input passage in at once, using the concept of Attention (\Citealt{vaswani2017attention}) along with positional encoding (to ensure you don’t start mixing up a sentence) to calculate the relative importance of each word. These results are then passed through a more traditional feedforward Neural Network (much faster than an RNN) and potentially another multi-head attention stage (applications differ) to produce your following piece of the passage. 

For the purposes of this research, we won’t be delving into the inner workings of the Attention mechanism, but essentially what it does is calculate the relevance of each word in the input. For example, if you have the input “I grew up in France… I speak fluent” and wish to predict the next word, the word “France” will be given a much higher score, indicating to the system what it should predict based on. This helps hugely with the long term dependency issue, since at each stage the entire input is considered, and thus there is a much lower risk of earlier information being “forgotten”, something that even the finest LSTMs struggled with. This could lead to performance issues, but thanks to the Attention mechanism and feedforward NN being used (allowing you to process the entire input at once), Transformers are in fact faster.

The best models also seem to utilise unsupervised learning (see Section~\ref{gpt}), which has the obvious benefit of being able to feed it much more data, as seen with GPT-2 and its 1.5 billion parameters (\Citealt{radford2019language}). However, you do need to feed it a massive amount of data for it to be in any way effective. 

A number of models have been developed with this architecture.


\subsubsection{BERT}

Google, being the organisation behind this original Attention mechanism research (\Citealt{vaswani2017attention}), was naturally one of the first to develop a Transformer architecture based on it. 

BERT (Bidirectional Encoder Representations from Transformers) was developed in 2018 and promptly achieved excellent results on natural language understanding tasks (\Citealt{devlin2018bert}). The findings were significant enough that in October 2019, Google began applying BERT to improve its search algorithms (\Citealt{nayak2019search}), the foundation of their business. 

However, while exciting and emblematic of the potential of Transformers, BERT is not necessarily suited to our task. As mentioned, it was mainly trained and used for the purposes of natural language understanding (NLU) – the clue being in the name "Bidirectional" – filling in missing words in sentences and machine translation (\Citealt{devlin2018bert}). For the purposes of our system, we are focused on NLG, so this isn't ideal.

\subsubsection{GPT \& GPT-2} \label{gpt}

GPT (Generative Pretrained Transformer) was also developed in 2018, this time by \href{https://openai.com/}{OpenAI}, a decoder-only Transformer architecture with largely unsupervised training. A massive corpus of text was gathered from the internet and fed to the model, which quickly produced impressive results (\Citealt{radford2018improving}). 

Their research focused on initially training the model (unsupervised) on a very diverse, wide range of text, then fine-tuning to each specific task on top of that, now supervised. 

It follows the simple principle of predicting \emph{the next word}, much like our phones will offer next-word suggestions based on what we have typed before, but on a much more advanced level and (ideally) not descending into gibberish. This was incredibly promising and looked ideal for my use case.

Having looked the potential of GPT for use in our system we next looked at its successor, GPT-2, taking things a step further with entirely unsupervised learning, and achieving even more state-of-the-art results than before (\Citealt{radford2019language}). To achieve this, a huge amount of parameters was necessary. For perspective, the smallest GPT model has parameters roughly equivalent in number to the largest BERT model. The largest GPT-2 has over an order of magnitude more than that. 1.5 billion parameters at its largest. This would seem to be the most appropriate architecture for our system.


% ---------------------------------------------------------------------------
%: ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------

