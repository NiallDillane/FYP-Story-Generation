% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Design} % top level followed by section, subsection
\label{product}

\ifpdf
    \graphicspath{{3_design/figures/PNG/}{3_design/figures/PDF/}{3_design/figures/}}
\else
    \graphicspath{{3_design/figures/EPS/}{3_design/figures/}}
\fi

% ----------------------- contents from here ------------------------


Our original intention was to take a procedural generation technique and modify the underlying architecture in some way. However, the enormity of this challenge was soon realised. These were concepts and models developed by teams of post-doctorate level researchers, working full-time on these problems, with the backing of huge corporations. For myself, just getting started in the field of machine learning and NLP, it didn't seem like the most impactful use of our time, or the most value we could contribute through our research. 

We took a step back, and considered where we might add value to the system. We discussed the fact that these models, despite being significant advancements and impressive research, were still nowhere near consistently challenging a human writer, and that curation of the AI productions was still required. This seemed like an area for exploration, to create an environment for human writers to work together with a language generation model, editing and inserting their own text as they go. 

We drew inspiration from Google Deepmind's AlphaGo (initially touched on in Section~\ref{introduction}), the AI board game player which challenged a top human player (\Citealt{borowiec2016alphago}). Two particular moments in this series of matches stood out to us: first, AlphaGo made a very unusual move, thought by human analysts to be so absurd it must be a mistake. However, this ended up being a pivotal move in the game, leading to the AI's victory. In another game following this, the human (Lee Sedol) made an unconventional move of his own, stunning viewers and apparently AlphaGo alike, as it led to his one and only win of the series.

This was an exhibition of creativity in the AI, unconventional thinking, not simply mimicking its taught behaviour and following the expected route. This was naturally of great interest to us, with the idea being our AI writer could come up with novel interpretations of its own. Even more interesting, though, was the human taking inspiration from the AI to be inventive in his own way. This was an exchange of creative spark, a collaborative process, both learning from each other, and ultimately what led us to pursue a collaborative writing environment. While AI may not be as competitive in the field of NLG as board games, the same principles of creativity persist.



\section{Language Model}

We identified a range of potential models for this project, from earlier grammars, to more recent Recurrent Neural Networks, and finally Transformers. For demonstration purposes, we decided it would be best to settle on one model.

\subsection{GPT-2} \label{design_gpt2}

As discussed in Section~\ref{gpt}, GPT-2 is currently the state-of-the-art model when it comes to the generation of natural language. A number of resources were identified which made it easier to utilise and fine-tune this model \footnote{\href{https://talktotransformer.com/}{Talk to Transformer} (\url{https://talktotransformer.com/}) by Adam King} \footnote{\href{https://github.com/minimaxir/gpt-2-simple}{GPT-2 Simple} (\url{https://github.com/minimaxir/gpt-2-simple}) by Max Woolf}, as well as the original open-source code itself \footnote{\href{https://github.com/openai/gpt-2/}{GPT-2} (\url{https://github.com/openai/gpt-2/}) by OpenAI}. 

This made the task somewhat less complex - our original intention being to modify the underlying architecture in some way. Still, training the model would be necessary, as well as a method of interacting with it. Since the code is open source, we set about reading through it, along with many explainer articles \footnote{\href{http://jalammar.github.io/illustrated-gpt2/}{The Illustrated GPT-2} (\url{http://jalammar.github.io/illustrated-gpt2/}) by Jay Alammar} and videos\footnote{\href{https://youtu.be/S0KakHcj_rs}{Attention Is All You Need} (\url{https://youtu.be/S0KakHcj\_rs/}) by the AI Socratic Circles, was helpful in understanding the Attention mechanism}.

As discussed in Section~\ref{product}, we ended up not modifying the architecture directly, but having this understanding was helpful in terms of understanding the output and elegantly building the system.

\subsection{Implementation} \label{modelImpl}

The implementation of GPT-2 I settled on was \href{https://huggingface.co/}{Huggingface}'s (\url{https://huggingface.co/}) repository of \href{https://huggingface.co/transformers/}{Transformers} (\url{https://huggingface.co/transformers/}). This contains not only GPT-2, but a collection of all state-of-the-art architectures, which leaves room for extensibility and including other models in the future.

The code provided in this repository allows for a variety of approaches for optimal accessibility: from simply running generation on a pre-trained model, to fine-tuning your own model, to custom generation or even altering the lower level code. A variety of scripts were included, making it quick and straightforward to get up and running. This seemed ideal for our purposes.

\section{Training} \label{training}

One concern that had prevailed since we delved into NNs was the hardware that is required to train these models. Transformers utilise Convolutional Neural Networks, which are faster than the Recurrent variant but still require significant computing power. Throughout the development process there was limited availability of, and accessibility to, hardware, which are especially important for the training phase.

Fortunately, alternatives do exist, and there are numerous cloud services available for training machine learning models on third-party hardware: \href{https://www.kaggle.com/}{Kaggle} (\url{https://www.kaggle.com/}), \href{https://lambdalabs.com/}{Lambda} (\url{https://lambdalabs.com/}) and \href{https://www.wandb.com/}{Weights \& Biases} (\url{https://www.wandb.com/}), just to name a few. There were drawbacks, namely cost, but these were promising.

Having analysed all the available alternatives, a decision was made to proceed with \href{https://colab.research.google.com/}{Google Colab} (\url{https://colab.research.google.com/}), which provides a Python Notebook interface and allows us to run code on advanced (NVIDIA Tesla K80) GPUs. There are limits on time and use cases, but we were comfortable we could fall within or work around these restrictions, especially considering that the product was free to use. Of course, if we wanted to train a model then we would need data.

\subsection{Data} \label{data}

Another issue that came up in the early stages was scope. Often an issue with projects of this nature, we wanted to make sure that we had a reasonable prototype which demonstrated progress. To generalise from the beginning would make the model's ability to optimise less clear, so we decided to focus on one type of story initially and perhaps generalise later, or at least leave that option open to ourselves.

We were careful when choosing a genre, since it was important to have a large and high quality data source to support it. Being a \href{https://www.reddit.com/}{Reddit} (\url{https://www.reddit.com/}) user for some time, I recognised the wealth of data available on the platform, not just as a whole, but categorised neatly into different sub-communities dedicated to certain topics. 

The \href{https://www.reddit.com/r/writingprompts}{Writing Prompts} (\url{https://www.reddit.com/r/writingprompts}) subreddit was the leading contender. A forum devoted to short story writing, it seemed like the perfect match. However, it was not specific to any genre of story and categorisation of the stories was minimal, meaning it was more of a general data source. Good, but not quite what we needed.

Having carefully analysed the alternatives, a decision was made to use \href{https://www.reddit.com/r/nosleep}{No Sleep} (\url{https://www.reddit.com/r/nosleep}), a community for short horror stories which, as the title goes, denied their users sleep. This was much more suitable, with focused data and a significant amount of it too. As of 26 March 2020, the subreddit has been going for 10 years, with 13.9 million subscribers, as well as Reddit's built in scoring system for rating the popularity (which can reasonably be correlated with quality) of each post. This is ideal for our use case, since we can limit our data to posts which have attained some degree of user approval.


\section{User Interface} \label{ui_design}

The user interface was a key aspect to the project, as we had decided to pursue a collaborative writing environment for the language model-based AI and human writer to work together in a live manner. Aiming to keep things minimal, we wanted the text to be the main focus, with parameter tuning and any buttons remaining subtle and in the background. A webapp was finalised as the most portable and adaptable solution, both for ease of development and use. Initial sketches looked something like Figure~\ref{ui_sketch}.

\figuremacroW{ui_sketch}{Initial UI Sketch}{Hand drawn}{0.8}

The architecture of the system was designed in line with Figure~\ref{architecture_sketch}, with the intention of separating concerns. The front end of the application would perform minimal processing, since the language model was anticipated to be quite heavy, and in a production scenario one could not expect the user to carry this burden. As such, calls would be made to an API (Application Programming Interface) to generate the next piece of text, based on the existing text and the parameters selected. This API would parse through and validate the input before making another call to the underlying Transformer model, formatting the result and passing it back to the front end.

\figuremacroW{architecture_sketch}{Early Architecture Sketch}{Hand drawn}{0.8}

\subsection{Back End} \label{backend}

With the generation script provided by the Huggingface implementation (mentioned in Section~\ref{modelImpl}), a minimal API was necessary, and for this the Python \href{https://flask.palletsprojects.com/}{Flask} (\url{https://flask.palletsprojects.com/}) framework was ideal. This is a minimal web framework that allows for easy exposition of API endpoints and interaction with front-end code via the request-response model and JSON objects.

\subsection{Front End} \label{frontend}

For the front end portion of the application we wished to use a modern framework and a reliable library for building UI elements. For these purposes, we found the \href{https://reactjs.org/}{React.js} (\url{https://reactjs.org/}) library to be ideal. This has become a recognised leader in the webapp development industry, particularly in data-driven applications as ours would be (\Citealt{krill2014react}). This meant the languages used on the front end would be JavaScript, HTML and CSS for styling. 

Initial layouts of the app were boxed out like Figure~\ref{front_boxed}, in line with our previous sketches of how the system would communicate in Figure~\ref{architecture_sketch}.

\figuremacroW{front_boxed}{Abstract layout of UI}{Hand drawn}{0.8}


%% here is how to do a page break
%\pagebreak
%%done
%
%\section{Quote}
% An in-text quote is declared in the following way:
%
%\begin{quote}
%
%Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward! Recycling is the way forward!    (CommonSense, p. 0) 
%
% \end{quote}
%
%Another way of doing it is:
%\begin{quote}
%\centering
%\emph{There are in our existence spots of time, \\
%That with distinct pre-eminence retain \\
%A renovating virtue, whence-depressed \\
%By false opinion and contentious thought, \\
%Or aught of heavier or more deadly weight, \\
%In trivial occupations, and the round \\
%Of ordinary intercourse-our minds \\ 
%Are nourished and invisibly repaired; \\ 
%A virtue, by which pleasure is enhanced, \\ 
%That penetrates, enables us to mount, \\ 
%When high, more high, and lifts us up when fallen. \\ 
%}
%\vspace{3mm}
%\raggedleft (\Citealt[verses 208-218]{poem})
%\end{quote}


%\section{Creating a list}
%
%\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
%\item \textbf{(2000)} item 1. 
%\item \textbf{(2004)} item 2. 
%\item \textbf{(2010)} item 3. 
%\item \textbf{(2013)} item 4. 
%\end{itemize}





% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
